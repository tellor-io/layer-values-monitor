"""Values monitor."""

import asyncio
import json
import logging
import os
import random
import time
from typing import Any

from layer_values_monitor.catchup import HeightTracker, get_current_height, process_missed_blocks
from layer_values_monitor.config_watcher import ConfigWatcher
from layer_values_monitor.constants import DENOM
from layer_values_monitor.custom_types import (
    AggregateReport,
    Metrics,
    Msg,
    NewReport,
    PendingPause,
    PowerThresholds,
    Reporter,
    ReporterQueryResponse,
)
from layer_values_monitor.discord import generic_alert
from layer_values_monitor.dispute import (
    determine_dispute_category,
    determine_dispute_fee,
    is_disputable,
)
from layer_values_monitor.evm_call import get_evm_call_trusted_value, get_web3_connection
from layer_values_monitor.saga_contract import SagaContractManager
from layer_values_monitor.telliot_feeds import fetch_value, get_feed, get_query
from layer_values_monitor.trb_bridge import decode_report_value, get_trb_bridge_trusted_value
from layer_values_monitor.utils import add_to_table, decode_hex_value

import aiohttp
import websockets
from telliot_feeds.datafeed import DataFeed


async def query_reporters(uri: str, logger: logging.Logger) -> ReporterQueryResponse | None:
    """Query all reporters and calculate total non-jailed power.

    Args:
        uri: RPC endpoint URI (e.g., "localhost:26657")
        logger: Logger instance

    Returns:
        ReporterQueryResponse with reporters list and total non-jailed power, or None if query fails

    """
    rpc_url = f"http://{uri}"

    # Query reporters using ABCI query
    payload = {
        "jsonrpc": "2.0",
        "method": "abci_query",
        "params": {"path": "/layer.reporter.Query/Reporters", "data": "", "prove": False},
        "id": 1,
    }

    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(rpc_url, json=payload) as response:
                if response.status == 200:
                    data = await response.json()

                    if "error" in data:
                        logger.error(f"RPC error querying reporters: {data['error']}")
                        return None

                    result = data.get("result")
                    if not result or "response" not in result:
                        logger.error("Invalid response structure when querying reporters")
                        return None

                    # The response.value contains base64 encoded protobuf data
                    # For now, we'll try a different approach using REST API if available
                    logger.debug("Trying alternative approach to query reporters...")
                    return await query_reporters_rest(uri, logger)

    except Exception as e:
        logger.error(f"Failed to query reporters via RPC: {e}")
        return None


async def query_reporters_rest(uri: str, logger: logging.Logger) -> ReporterQueryResponse | None:
    """Query reporters using REST API endpoint.

    Args:
        uri: Base URI (e.g., "localhost:26657")
        logger: Logger instance

    Returns:
        ReporterQueryResponse with reporters list and total non-jailed power, or None if query fails

    """
    # Try common REST API ports
    rest_ports = ["1317", "1316", "26617"]
    base_host = uri.split(":")[0] if ":" in uri else uri

    for port in rest_ports:
        rest_url = f"http://{base_host}:{port}/layer/reporter/reporters"

        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(rest_url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                    if response.status == 200:
                        data = await response.json()
                        return parse_reporters_response(data, logger)

        except Exception as e:
            logger.debug(f"Failed to query reporters via REST on port {port}: {e}")
            continue

    logger.error("Failed to query reporters via all available methods")
    return None


def parse_reporters_response(data: dict[str, Any], logger: logging.Logger) -> ReporterQueryResponse | None:
    """Parse the reporters query response and calculate total non-jailed power.

    Args:
        data: Response data from reporters query
        logger: Logger instance

    Returns:
        ReporterQueryResponse with parsed reporters and total power

    """
    try:
        reporters_data = data.get("reporters", [])
        reporters = []
        total_non_jailed_power = 0

        for reporter_data in reporters_data:
            address = reporter_data.get("address", "")
            power = int(reporter_data.get("power", "0"))

            # Check if reporter is jailed
            metadata = reporter_data.get("metadata", {})
            jailed = metadata.get("jailed", False)
            moniker = metadata.get("moniker", "")

            reporter = Reporter(address=address, power=power, jailed=jailed, moniker=moniker)
            reporters.append(reporter)

            # Add to total power if not jailed
            if not jailed:
                total_non_jailed_power += power

        logger.info(f"Queried {len(reporters)} reporters, total non-jailed power: {total_non_jailed_power}")

        return ReporterQueryResponse(reporters=reporters, total_non_jailed_power=total_non_jailed_power)

    except Exception as e:
        logger.error(f"Failed to parse reporters response: {e}")
        return None


async def listen_to_websocket_events(
    uri: str, queries: list[str], q: asyncio.Queue, logger: logging, height_tracker: HeightTracker
) -> None:
    """Connect to a layer websocket and fetch events, adding them to queue for monitoring.

    uri: address to chain (ie localhost:26657)
    queries: list of query strings to subscribe to
    q: queue to store raw response for later processing
    logger: logger instance
    height_tracker: height tracker to detect missed blocks
    """
    logger.info(f"üí° Starting WebSocket connection for {len(queries)} subscriptions...")

    # Prepare subscription messages
    subscription_messages = []
    for i, query_string in enumerate(queries, 1):
        subscription_messages.append(
            json.dumps({"jsonrpc": "2.0", "method": "subscribe", "id": i, "params": {"query": query_string}})
        )

    ws_uri = f"ws://{uri}/websocket"
    retry_count = 0
    base_delay = 1
    max_delay = 60

    while True:
        try:
            logger.info(f"üí° Connecting to WebSocket at {ws_uri}... (Attempt {retry_count + 1})")
            async with websockets.connect(ws_uri) as websocket:
                # Send all subscription messages
                for i, msg in enumerate(subscription_messages):
                    await websocket.send(msg)
                    logger.info(f"‚úÖ Sent subscription {i + 1}: {queries[i]}")

                # Reset retry count on successful connection
                retry_count = 0
                logger.info("‚úÖ WebSocket connection established successfully")
                while True:
                    response = await websocket.recv()
                    parsed_response = json.loads(response)

                    # Log EVERYTHING to see what we're getting
                    logger.info(f"üîµ Raw WebSocket response: {json.dumps(parsed_response)[:500]}")

                    await q.put(parsed_response)

        except websockets.ConnectionClosed as e:
            logger.warning(f"WebSocket connection closed: {e}")
        except websockets.WebSocketException as e:
            logger.error(f"WebSocket error: {e}")
        except Exception as e:
            logger.error(f"Unexpected error: {e}", exc_info=True)

        logger.info("going through the retry phase since connection was closed")

        # Process missed blocks on reconnection
        if retry_count == 0:  # Only on first reconnection attempt
            try:
                # Get current height from RPC
                current_height = await get_current_height(uri)
                if current_height:
                    missed_range = height_tracker.get_missed_range(current_height)
                    if missed_range:
                        start_height, end_height = missed_range
                        total_missed = end_height - start_height + 1

                        # Log if we're limiting catch-up
                        if total_missed > height_tracker.max_catchup_blocks:
                            logger.warning(
                                f"‚ö†Ô∏è CATCHUP LIMITED - {total_missed} blocks missed, "
                                f"processing only last {height_tracker.max_catchup_blocks} blocks "
                                f"({start_height}-{end_height}) to prevent stale price comparisons"
                            )
                        else:
                            logger.info(f"‚úÖ Processing {total_missed} missed blocks ({start_height}-{end_height})")

                        await process_missed_blocks(uri, start_height, end_height, q, logger)
                        height_tracker.update(current_height)
                        logger.info("‚úÖ Catch-up processing completed successfully")
            except Exception as e:
                logger.error(f"Failed to process missed blocks: {e}")

        # Calculate delay with exponential backoff and jitter for reconnection
        delay = min(base_delay * (2**retry_count), max_delay)
        # Add jitter to prevent thundering herd problem
        jitter = random.uniform(0, 0.1 * delay)
        actual_delay = delay + jitter

        retry_count += 1
        logger.info(f"Reconnecting in {actual_delay:.2f} seconds (retry {retry_count})")
        await asyncio.sleep(actual_delay)


async def raw_data_queue_handler(
    raw_data_q: asyncio.Queue,
    new_reports_q: asyncio.Queue,
    agg_reports_q: asyncio.Queue | None,
    logger: logging,
    height_tracker: HeightTracker,
    max_iterations: float = float("inf"),  # use iterations var for testing purposes instead of using a while loop
) -> None:
    """Process raw WebSocket data and route to appropriate queues.

    Handles both new reports and aggregate reports from the same WebSocket stream.
    New reports are collected by height and sent to new_reports_q.
    Aggregate reports are sent directly to agg_reports_q.
    """
    logger.debug("Processing raw WebSocket data...")
    iterations = 0
    reports_collections: dict[str, list[NewReport]] = {}
    # Smart batching configuration
    MAX_BATCH_SIZE = 25  # Process if we hit this many reports
    BATCH_TIMEOUT = 5.0  # Process after 5 seconds regardless
    # current height being collected for - start as None to detect first height
    current_height = None
    last_batch_time = time.time()
    while iterations < max_iterations:
        iterations += 1
        logger.debug(f"DEBUG: Starting iteration {iterations}, waiting for raw_data...")
        raw_data: dict[str, Any] = await raw_data_q.get()
        raw_data_q.task_done()
        logger.debug(f"DEBUG: Got raw_data for iteration {iterations}")
        logger.debug(f"DEBUG: Raw data keys: {list(raw_data.keys())}")

        # Log everything that comes through the queue
        result: dict[str, Any] = raw_data.get("result")
        result_type = (
            "missing" if result is None else ("empty" if result == {} else f"present with keys: {list(result.keys())}")
        )
        logger.info(f"üì• Queue data - keys: {list(raw_data.keys())}, id: {raw_data.get('id')}, result: {result_type}")

        if result is None:
            logger.debug("DEBUG: No result in raw_data, skipping")
            continue

        # Skip subscription acknowledgments (empty result)
        if result == {}:
            logger.debug("DEBUG: Subscription acknowledgment, skipping")
            continue

        logger.debug(f"DEBUG: Result keys: {list(result.keys()) if result else 'None'}")

        # Filter out duplicate events: Tendermint sends the same aggregate reports in both
        # 'NewBlock' and 'NewBlockEvents'. We only process 'NewBlockEvents' to avoid duplicates.
        data = result.get("data", {})
        event_type = data.get("type")
        logger.debug(f"DEBUG: Event type from data: {event_type}")
        if event_type == "tendermint/event/NewBlock":
            logger.debug("DEBUG: Skipping NewBlock event")
            continue

        events = result.get("events")
        logger.debug(
            f"DEBUG: Events in result: {events is not None}, event keys: {list(events.keys()) if events else 'None'}"
        )
        if events is None:
            logger.debug("DEBUG: No events in result, skipping")
            continue

        # Determine event type by presence of keys
        is_new_report = "new_report.query_id" in events
        is_agg_report = "aggregate_report.query_id" in events or "aggregate_report.aggregate_power" in events

        logger.debug(f"DEBUG: Event type check - is_new_report: {is_new_report}, is_agg_report: {is_agg_report}")
        if events:
            logger.debug(f"DEBUG: Sample event keys: {list(events.keys())[:10]}")  # Show first 10 keys

        if is_new_report:
            logger.info(
                f"‚úÖ Detected new_report event with query_id: {events.get('new_report.query_id', ['unknown'])[0][:16]}"
            )
            try:
                # get current height from event
                height = events["tx.height"][0]
                height = int(height)
                report = NewReport(
                    query_type=events["new_report.query_type"][0],
                    query_data=events["new_report.query_data"][0],
                    query_id=events["new_report.query_id"][0],
                    value=events["new_report.value"][0],
                    aggregate_method=events["new_report.aggregate_method"][0],
                    cyclelist=events["new_report.cyclelist"][0],
                    power=events["new_report.reporter_power"][0],
                    reporter=events["new_report.reporter"][0],
                    timestamp=events["new_report.timestamp"][0],
                    meta_id=events["new_report.meta_id"][0],
                    tx_hash=events["tx.hash"][0],
                )

                # Add the current report to the collection for the current height
                reports_collected = reports_collections.get(report.query_id, None)
                if reports_collected is None:
                    reports_collections[report.query_id] = [report]
                else:
                    reports_collections[report.query_id].append(report)

                logger.info(
                    f"DEBUG: Added report - Height: {height}, Current Height: {current_height}, Collection Size: {len(reports_collections)}"
                )

                # Smart batching logic: process reports based on multiple conditions
                should_process = False
                process_reason = ""

                # Height has incremented (process previous height's reports)
                if current_height is not None and height > current_height:
                    should_process = True
                    process_reason = f"height increment ({current_height} -> {height})"
                    logger.info(f"DEBUG: Height increment detected - {current_height} -> {height}")

                # Batch size threshold reached
                elif len(reports_collections) >= MAX_BATCH_SIZE:
                    should_process = True
                    process_reason = f"batch size limit ({len(reports_collections)} >= {MAX_BATCH_SIZE})"
                    logger.info(f"DEBUG: Batch size limit reached - {len(reports_collections)} >= {MAX_BATCH_SIZE}")

                # Timeout reached
                elif time.time() - last_batch_time > BATCH_TIMEOUT:
                    should_process = True
                    process_reason = f"timeout ({time.time() - last_batch_time:.1f}s > {BATCH_TIMEOUT}s)"
                    logger.info(f"DEBUG: Timeout reached - {time.time() - last_batch_time:.1f}s > {BATCH_TIMEOUT}s")

                # First report (current_height is None) - process immediately
                elif current_height is None:
                    should_process = True
                    process_reason = "first report"
                    logger.info("DEBUG: First report detected - current_height is None")

                logger.info(f"DEBUG: Processing decision - should_process: {should_process}, reason: {process_reason}")

                if should_process and len(reports_collections) > 0:
                    total_reports = sum(len(reports) for reports in reports_collections.values())
                    query_counts = [f"{query_id[:12]}:{len(reports)}" for query_id, reports in reports_collections.items()]

                    collected_height = current_height if current_height is not None else height
                    logger.info(
                        f"Processing {total_reports} reports collected at height {collected_height} ({process_reason}), qIds: [{', '.join(query_counts)}]"
                    )

                    logger.info(f"DEBUG: About to send {len(reports_collections)} query collections to queue")
                    await new_reports_q.put(dict(reports_collections))
                    reports_collections.clear()
                    last_batch_time = time.time()
                    logger.info("DEBUG: Cleared reports_collections and updated last_batch_time")

                # Update current height (either first time or after processing)
                if current_height is None or height > current_height:
                    old_height = current_height
                    current_height = height
                    logger.info(f"DEBUG: Updated current_height from {old_height} to {current_height}")
                    # Track height for missed block detection
                    height_tracker.update(height)

            except (KeyError, IndexError) as e:
                logger.warning(f"malformed new_report returned by websocket: {e.__str__()}")
                continue

        elif is_agg_report and agg_reports_q is not None:
            # This ensures new reports are processed for disputes even when aggregate arrives at same height
            if len(reports_collections) > 0:
                total_reports = sum(len(reports) for reports in reports_collections.values())
                query_counts = [f"{query_id[:12]}:{len(reports)}" for query_id, reports in reports_collections.items()]

                logger.info(
                    f"Processing {total_reports} reports collected at height {height}, qIds: [{', '.join(query_counts)}]"
                )

            try:
                height = current_height

                agg_report = AggregateReport(
                    query_id=events["aggregate_report.query_id"][0],
                    query_data=events["aggregate_report.query_data"][0],
                    value=events["aggregate_report.value"][0],
                    aggregate_power=events["aggregate_report.aggregate_power"][0],
                    micro_report_height=events["aggregate_report.micro_report_height"][0],
                    height=height,
                )

                # Track height for missed block detection
                if height:
                    height_tracker.update(height)

                await agg_reports_q.put(agg_report)
            except (KeyError, IndexError) as e:
                logger.warning(f"malformed aggregate_report returned by websocket: {e.__str__()}")
                continue
        else:
            # Unknown or not interested
            continue

    # Cleanup: Process any remaining reports before function exits
    if len(reports_collections) > 0:
        total_reports = sum(len(reports) for reports in reports_collections.values())
        query_counts = [f"{query_id[:12]}:{len(reports)}" for query_id, reports in reports_collections.items()]

        collected_height = current_height if current_height is not None else "unknown"
        logger.info(
            f"Processing {total_reports} remaining reports collected at height {collected_height} (cleanup), qIds: [{', '.join(query_counts)}]"
        )

        await new_reports_q.put(dict(reports_collections))
        reports_collections.clear()


async def inspect_reports(
    reports: list[NewReport],
    disputes_q: asyncio.Queue,
    config_watcher: ConfigWatcher,
    logger: logging,
) -> None:
    """Fetch value for a query id and check it against the list of reports' values and determine if any are disputable.

    reports: list of new_reports for a query id
    disputes_q: the queue to add disputes to for dispute processing
    config_watcher: live config for query ids threshold settings
    """
    if len(reports) == 0:
        return None
    query_data = reports[0].query_data
    query_id = reports[0].query_id
    query_type = reports[0].query_type

    logger.info(f"CONFIG DEBUG: inspect_reports called for query_id: {query_id[:16]}..., query_type: {query_type}")

    # Check if query type is supported
    logger.info(f"CONFIG DEBUG: Checking if query type '{query_type}' is supported...")
    if not config_watcher.is_supported_query_type(query_type):
        logger.warning(f"CONFIG DEBUG: Query type '{query_type}' is NOT supported")
        # Extract asset pair if possible for better logging
        asset_pair = "Unknown"
        try:
            query_obj = get_query(query_data)
            if query_obj and hasattr(query_obj, "asset") and hasattr(query_obj, "currency"):
                asset_pair = f"{query_obj.asset}/{query_obj.currency}"
        except Exception:
            pass

        # Send Discord alert for unknown query type
        await send_unknown_query_type_alert(query_type, query_id, asset_pair, reports[0], logger)
        return None
    else:
        logger.info(f"CONFIG DEBUG: Query type '{query_type}' IS supported")

    # Get metrics configuration using new method
    logger.info(f"CONFIG DEBUG: Getting metrics for query_id: {query_id[:16]}..., query_type: {query_type}")
    metrics = config_watcher.get_metrics_for_query(query_id, query_type)
    if metrics is None:
        logger.error(f"CONFIG DEBUG: Unable to get metrics for query {query_id[:16]}... of type {query_type}")
        return None
    else:
        logger.info(f"CONFIG DEBUG: Successfully got metrics: {metrics}")

    # Branch based on query type - each type has its own inspection path
    logger.info(f"üîÄ Routing to {query_type} inspection path...")

    if query_type.lower() == "spotprice":
        return await inspect_spotprice_path(
            reports, disputes_q, config_watcher, query_id, query_data, query_type, metrics, logger
        )
    elif query_type.lower() == "evmcall":
        return await inspect_evmcall_path(reports, disputes_q, query_id, query_data, metrics, logger)
    elif query_type.lower() == "trbbridge":
        return await inspect_trbbridge_path(reports, disputes_q, query_id, metrics, logger)
    else:
        logger.error(f"‚ùå No inspection path defined for query type: {query_type}")
        return None


# ====================================================================================
# QUERY TYPE SPECIFIC INSPECTION PATHS
# ====================================================================================


async def inspect_spotprice_path(
    reports: list[NewReport],
    disputes_q: asyncio.Queue,
    config_watcher: ConfigWatcher,
    query_id: str,
    query_data: str,
    query_type: str,
    metrics: Metrics,
    logger: logging,
) -> None:
    """SpotPrice inspection path with 4 scenarios:
    1. In config AND in telliot ‚Üí proceed normally
    2. In telliot but NOT in config ‚Üí alert + proceed with global defaults
    3. NOT in telliot and NOT in config ‚Üí alert + skip (foreign query)
    4. In config but NOT in telliot ‚Üí NEW alert + skip (can't get trusted value)
    """
    logger.info(f"üìä SpotPrice inspection - QueryID: {query_id[:16]}...")

    # Step 1: Check if query is in our config
    query_type_configs = config_watcher.query_configs.get(query_type.lower(), {})
    has_specific_config = query_id.lower() in query_type_configs
    logger.info(f"üìù Config check: has_specific_config = {has_specific_config}")

    # Step 2: Check if query is in telliot catalog
    from telliot_feeds.queries.query_catalog import query_catalog

    catalog_entry = query_catalog.find(query_id=query_id)
    in_telliot = len(catalog_entry) > 0
    logger.info(f"üìö Telliot check: in_telliot = {in_telliot}")

    # Step 3: Extract asset pair and decode value (for alerts)
    asset_pair = "Unknown"
    decoded_value = reports[0].value
    try:
        query_obj = get_query(query_data)
        if query_obj:
            if hasattr(query_obj, "asset") and hasattr(query_obj, "currency"):
                asset_pair = f"{query_obj.asset}/{query_obj.currency}"
            try:
                decoded_value = query_obj.value_type.decode(bytes.fromhex(reports[0].value))
            except Exception:
                pass
    except Exception:
        pass

    # Step 4: Handle the 4 scenarios
    logger.info(f"üîÄ Scenario: has_config={has_specific_config}, in_telliot={in_telliot}")

    # Scenario 3: NOT in config AND NOT in telliot ‚Üí Foreign query (SKIP)
    if not has_specific_config and not in_telliot:
        logger.warning("‚ùå Foreign query - not in config OR telliot")
        await send_foreign_query_alert(query_id, query_type, asset_pair, reports[0], logger)
        return None  # Skip inspection - no trusted value available

    # Scenario 4: IN config but NOT in telliot ‚Üí NEW ALERT (SKIP - can't get trusted value)
    elif has_specific_config and not in_telliot:
        logger.warning("üÜï Query in config but NOT in telliot catalog")
        alert_msg = "üÜï **Query found in our config, but not in telliot catalog**\n"
        alert_msg += f"**QueryId:** {query_id}\n"
        alert_msg += f"**QueryType:** {query_type}\n"
        if asset_pair != "Unknown":
            alert_msg += f"**Asset pair:** {asset_pair}\n"
        alert_msg += f"**Value:** {decoded_value}\n"
        alert_msg += f"**Reporter:** {reports[0].reporter}\n"
        alert_msg += f"**Tx Hash:** {reports[0].tx_hash}\n"
        alert_msg += "**Status:** Cannot inspect - telliot unavailable for trusted value"

        logger.info(f"Config-but-not-telliot alert:\n{alert_msg}")
        generic_alert(alert_msg)
        return None  # Can't get trusted value from telliot

    # Scenario 2: NOT in config but IN telliot ‚Üí Unconfigured query (PROCEED with global defaults)
    elif not has_specific_config and in_telliot:
        logger.warning("‚ö†Ô∏è Unconfigured query - in telliot but NOT in config")

        # Check if this query type typically has per-query configs
        has_specific_configs = any(k != "defaults" for k in query_type_configs.keys())

        if has_specific_configs:
            # Send alert
            alert_msg = "‚ö†Ô∏è **Query found in telliot, but not in our config file**\n"
            alert_msg += f"**QueryId:** {query_id}\n"
            alert_msg += f"**QueryType:** {query_type}\n"
            if asset_pair != "Unknown":
                alert_msg += f"**Asset pair:** {asset_pair}\n"
            alert_msg += f"**Value:** {decoded_value}\n"
            alert_msg += f"**Reporter:** {reports[0].reporter}\n"
            alert_msg += f"**Tx Hash:** {reports[0].tx_hash}\n"
            alert_msg += (
                f"**Status:** Using global defaults (alert={metrics.alert_threshold}, warning={metrics.warning_threshold})"
            )

            logger.info(f"Unconfigured query alert:\n{alert_msg}")
            generic_alert(alert_msg)
        # Fall through to proceed with inspection using global defaults

    # Scenario 1: IN config AND IN telliot ‚Üí Normal path (PROCEED)
    else:  # has_specific_config and in_telliot
        logger.info("‚úÖ Normal path - query in both config and telliot")
        # Fall through to proceed with inspection

    # Step 5: Proceed with telliot inspection (for scenarios 1 and 2)
    logger.info("üîç Starting telliot inspection...")

    query = get_query(query_data)
    if query is None:
        logger.error("‚ùå Unable to get query object")
        return None
    logger.info(f"‚úÖ Query object retrieved: {query.__class__.__name__}")

    feed = await get_feed(query_id, query, logger)
    if feed is None:
        logger.error("‚ùå Unable to get feed")
        return None
    logger.info("‚úÖ Feed retrieved successfully")

    logger.info("üì° Fetching trusted value from telliot...")
    trusted_value, _ = await fetch_value(feed)
    if trusted_value is None:
        logger.error("‚ùå Unable to fetch trusted value")
        return None
    logger.info(f"‚úÖ Trusted value fetched: {trusted_value}")

    # Create fetcher lambda for double-check logic
    async def fetch_trusted_value():
        return await fetch_value(feed)

    logger.info(f"üîé Inspecting {len(reports)} report(s)...")
    for i, r in enumerate(reports, 1):
        logger.info(f"üìä Report {i}/{len(reports)} - Reporter: {r.reporter}")
        reported_value = query.value_type.decode(bytes.fromhex(r.value))
        logger.info(f"‚úÖ Decoded value: {reported_value}")
        await inspect(r, reported_value, trusted_value, disputes_q, metrics, logger, query=query, trusted_value_fetcher=fetch_trusted_value)

    logger.info("‚úÖ SpotPrice inspection completed")
    return None


async def inspect_evmcall_path(
    reports: list[NewReport],
    disputes_q: asyncio.Queue,
    query_id: str,
    query_data: str,
    metrics: Metrics,
    logger: logging,
) -> None:
    """EVMCall inspection path."""
    logger.info(f"‚öôÔ∏è EVMCall inspection - QueryID: {query_id[:16]}...")

    # Get query object
    query = get_query(query_data)
    if query is None:
        logger.error("‚ùå Unable to get query object for EVMCall")
        return None
    logger.info(f"‚úÖ Query object retrieved: {query.__class__.__name__}")

    # Get feed
    feed = await get_feed(query_id, query, logger)
    if feed is None:
        logger.error("‚ùå Unable to get feed for EVMCall")
        return None
    logger.info("‚úÖ Feed retrieved successfully")

    # Call existing implementation
    logger.info(f"üîé Inspecting {len(reports)} EVMCall report(s)...")
    return await inspect_evm_call_reports(reports, feed, disputes_q, query_id, metrics, logger)


async def inspect_trbbridge_path(
    reports: list[NewReport],
    disputes_q: asyncio.Queue,
    query_id: str,
    metrics: Metrics,
    logger: logging,
) -> None:
    """TRBBridge inspection path."""
    logger.info(f"üåâ TRBBridge inspection - QueryID: {query_id[:16]}...")
    logger.info(f"üîé Inspecting {len(reports)} TRBBridge report(s)...")

    # Call existing implementation
    return await inspect_trbbridge_reports(reports, disputes_q, query_id, metrics, logger)


async def new_reports_queue_handler(
    new_reports_q: asyncio.Queue,
    disputes_q: asyncio.Queue,
    config_watcher: ConfigWatcher,
    logger: logging,
) -> None:
    """Handle new reports from the queue and process them."""
    running_tasks = set()
    task_cleanup_threshold = 1000  # Safety threshold for task cleanup

    while True:
        new_reports: dict = await new_reports_q.get()

        # Periodic cleanup of completed tasks to prevent memory growth
        if len(running_tasks) > task_cleanup_threshold:
            completed_tasks = {t for t in running_tasks if t.done()}
            running_tasks -= completed_tasks
            logger.warning(f"üßπ Cleaned up {len(completed_tasks)} completed tasks (active: {len(running_tasks)})")

        for report in new_reports.values():
            # Create a task for each query id
            task = asyncio.create_task(inspect_reports(report, disputes_q, config_watcher, logger))

            # Add task tracking and cleanup
            running_tasks.add(task)
            task.add_done_callback(running_tasks.discard)

            # Handle exceptions
            def handle_task_exception(t: asyncio.Task) -> None:
                if t.exception():
                    logger.error(f"Task raised exception: {t.exception()}")

            task.add_done_callback(handle_task_exception)

        new_reports_q.task_done()


async def inspect_aggregate_report(
    agg_report: AggregateReport,
    config_watcher: ConfigWatcher,
    logger: logging,
    uri: str | None = None,
    power_thresholds: PowerThresholds | None = None,
) -> tuple[bool, str, dict[str, Any] | None] | None:
    """Inspect an aggregate report using the same logic as individual reports."""
    query_id = agg_report.query_id
    query_data = agg_report.query_data

    # Determine query type from query_data (same logic as inspect_reports)
    query = get_query(query_data)
    if query is None:
        logger.error(f"Unable to parse query data for aggregate report query id: {query_id}")
        return None

    # Get query type from the query object
    query_type = query.__class__.__name__
    logger.info(f"CONFIG DEBUG: Aggregate report query type determined: {query_type}")

    # Check if query type is supported
    if not config_watcher.is_supported_query_type(query_type):
        logger.warning(f"CONFIG DEBUG: Aggregate report query type '{query_type}' is NOT supported")
        return None

    # Get configuration using the actual query type
    metrics = config_watcher.get_metrics_for_query(query_id, query_type)
    if metrics is None:
        logger.warning(f"No configuration available for aggregate report query {query_id[:16]}... of type {query_type}")
        return None

    # Get feed and trusted value (same logic as inspect_reports)
    query = get_query(query_data)
    if query is None:
        logger.error(f"Unable to parse query data for aggregate report query id: {query_id}")
        return None

    feed = await get_feed(query_id, query, logger)
    if feed is None:
        logger.error(f"Unable to get feed for aggregate report query id: {query_id}")
        return None

    trusted_value, _ = await fetch_value(feed)
    if trusted_value is None:
        logger.error(f"Unable to fetch trusted value for aggregate report query id: {query_id}")
        return None

    # Decode the aggregate hex value
    try:
        reported_value = decode_hex_value(agg_report.value)
    except (OverflowError, ValueError) as e:
        logger.error(f"üö® Skipping aggregate report - invalid hex value: {e}")
        return None

    # Check if disputable using same logic
    alertable, disputable, diff = is_disputable(
        metrics.metric,
        metrics.alert_threshold,
        metrics.warning_threshold,
        reported_value,
        trusted_value,
        logger=logger,
    )

    if alertable is None:
        return None

    # Determine severity and action
    should_pause = False
    reason = ""
    power_info = None

    # Check if deviation exceeds pause threshold
    if diff >= metrics.pause_threshold:
        # Use power-based logic if Saga guard is enabled (both uri and power_thresholds provided)
        if uri and power_thresholds:
            reason = (
                f"Aggregate report deviation ({diff:.4f}) exceeds pause threshold "
                f"({metrics.pause_threshold:.4f}) - EVALUATING POWER THRESHOLDS"
            )

            power_info = await calculate_power_percentage(agg_report, uri, power_thresholds, logger)
            if power_info:
                should_pause = power_info["should_pause_immediately"]
                if power_info["should_pause_immediately"]:
                    immediate_threshold = power_thresholds.immediate_pause_threshold * 100
                    reason += (
                        f" | Power: {power_info['power_percentage']:.1f}% (>{immediate_threshold}%) - PAUSE IMMEDIATELY"
                    )
                elif power_info["should_pause_delayed"]:
                    delayed_threshold = power_thresholds.delayed_pause_threshold * 100
                    delay_hours = power_thresholds.pause_delay_hours
                    reason += (
                        f" | Power: {power_info['power_percentage']:.1f}% "
                        f"(>{delayed_threshold}%) - PAUSE AFTER {delay_hours}H DELAY"
                    )
                else:
                    should_pause = False
                    delayed_threshold = power_thresholds.delayed_pause_threshold * 100
                    reason += f" | Power: {power_info['power_percentage']:.1f}% (<{delayed_threshold}%) - NO PAUSE"
            else:
                # Fallback to immediate pause if power calculation fails
                should_pause = True
                reason += " | Power calculation failed - DEFAULTING TO IMMEDIATE PAUSE"
        else:
            # Traditional logic when Saga guard is not enabled or not properly configured
            should_pause = True
            reason = (
                f"Aggregate report deviation ({diff:.4f}) exceeds pause threshold "
                f"({metrics.pause_threshold:.4f}) - CIRCUIT BREAKER ACTIVATED"
            )

    elif disputable and metrics.warning_threshold > 0:
        # Use same category determination logic for non-pause alerts
        category = determine_dispute_category(
            category_thresholds={
                "major": metrics.major_threshold,
                "minor": metrics.minor_threshold,
                "warning": metrics.warning_threshold,
            },
            diff=diff,
        )
        reason = f"Aggregate report deviation ({diff:.4f}) exceeds {category} threshold"
    elif alertable:
        reason = f"Aggregate report deviation ({diff:.4f}) exceeds alert threshold"
    else:
        reason = f"acceptable deviation: {diff:.4f}"

    # Return appropriate format based on whether power thresholds were used
    if uri and power_thresholds:
        return should_pause, reason, power_info
    else:
        return should_pause, reason


async def calculate_power_percentage(
    agg_report: AggregateReport,
    uri: str,
    power_thresholds: PowerThresholds,
    logger: logging.Logger,
) -> dict[str, Any] | None:
    """Calculate the power percentage of an aggregate report relative to total non-jailed power.

    Args:
        agg_report: The aggregate report to analyze
        uri: RPC endpoint URI
        power_thresholds: Power threshold configuration
        logger: Logger instance

    Returns:
        Dict with power analysis results or None if query fails

    """
    try:
        # Query all reporters
        reporters_response = await query_reporters(uri, logger)
        if not reporters_response:
            logger.error("Failed to query reporters for power calculation")
            return None

        # Get aggregate power from the report
        aggregate_power = int(agg_report.aggregate_power)
        total_power = reporters_response.total_non_jailed_power

        if total_power == 0:
            logger.error("Total non-jailed power is 0 - cannot calculate percentage")
            return None

        # Calculate power percentage
        power_percentage = (aggregate_power / total_power) * 100.0

        # Determine pause decisions based on thresholds
        should_pause_immediately = power_percentage > (power_thresholds.immediate_pause_threshold * 100)
        should_pause_delayed = power_percentage > (power_thresholds.delayed_pause_threshold * 100)

        logger.info(
            f"Power analysis: aggregate={aggregate_power}, total={total_power}, "
            f"percentage={power_percentage:.2f}%, immediate_pause={should_pause_immediately}, "
            f"delayed_pause={should_pause_delayed}"
        )

        return {
            "aggregate_power": aggregate_power,
            "total_power": total_power,
            "power_percentage": power_percentage,
            "should_pause_immediately": should_pause_immediately,
            "should_pause_delayed": should_pause_delayed,
            "delay_hours": power_thresholds.pause_delay_hours,
            "immediate_threshold": power_thresholds.immediate_pause_threshold * 100,
            "delayed_threshold": power_thresholds.delayed_pause_threshold * 100,
        }

    except Exception as e:
        logger.error(f"Error calculating power percentage: {e}")
        return None


async def process_pending_pauses(
    pending_pauses: dict[str, PendingPause],
    saga_contract_manager: SagaContractManager | None,
    logger: logging.Logger,
    current_time: float,
) -> None:
    """Process pending pauses that have reached their delay timeout.

    Args:
        pending_pauses: Dictionary of pending pauses by query_id
        saga_contract_manager: Saga contract manager for pausing
        logger: Logger instance
        current_time: Current timestamp

    """
    expired_pauses = []

    for query_id, pending_pause in pending_pauses.items():
        delay_seconds = pending_pause.power_info["delay_hours"] * 3600
        if current_time >= pending_pause.trigger_time + delay_seconds:
            expired_pauses.append(query_id)

            logger.critical(
                f"üïê DELAYED PAUSE TRIGGERED - {pending_pause.reason} - "
                f"Delay period ({pending_pause.power_info['delay_hours']}h) expired"
            )

            # Execute the pause
            if saga_contract_manager and pending_pause.contract_address != "0x0000000000000000000000000000000000000000":
                tx_hash, status = await saga_contract_manager.pause_contract(
                    pending_pause.contract_address, pending_pause.query_id
                )

                if tx_hash:
                    if status == "success":
                        logger.critical(f"üö® DELAYED CONTRACT PAUSED SUCCESSFULLY - TxHash: {tx_hash}")
                    elif status == "timeout":
                        logger.warning(f"‚è∞ DELAYED PAUSE TRANSACTION PENDING - TxHash: {tx_hash}")
                else:
                    if status == "already_paused":
                        logger.warning(f"‚ö†Ô∏è CONTRACT ALREADY PAUSED - Address: {pending_pause.contract_address}")
                    else:
                        logger.error(
                            f"‚ùå DELAYED PAUSE FAILED - Address: {pending_pause.contract_address}, Status: {status}"
                        )
            else:
                logger.warning("‚ö†Ô∏è DELAYED PAUSE SKIPPED - Saga contract manager not available")

    # Remove expired pauses
    for query_id in expired_pauses:
        del pending_pauses[query_id]

    if expired_pauses:
        logger.info(f"Processed {len(expired_pauses)} expired pending pauses")


async def execute_contract_pause(
    saga_contract_manager: SagaContractManager | None,
    agg_report: AggregateReport,
    config_watcher: ConfigWatcher,
    logger: logging.Logger,
) -> None:
    """Execute contract pause for an aggregate report.

    Args:
        saga_contract_manager: Saga contract manager for pausing
        agg_report: The aggregate report triggering the pause
        config_watcher: Configuration watcher
        logger: Logger instance

    """
    if saga_contract_manager is not None:
        # Get contract address from config
        config = config_watcher.get_config()
        query_config = config.get(agg_report.query_id.lower())

        if query_config and query_config.get("datafeed_ca"):
            contract_address = query_config.get("datafeed_ca")

            # Skip if placeholder address
            if contract_address != "0x0000000000000000000000000000000000000000":
                tx_hash, status = await saga_contract_manager.pause_contract(contract_address, agg_report.query_id)
                if tx_hash:
                    if status == "success":
                        logger.critical(f"üö® CONTRACT PAUSED SUCCESSFULLY - TxHash: {tx_hash}")
                    elif status == "timeout":
                        logger.warning(f"‚è∞ PAUSE TRANSACTION PENDING - TxHash: {tx_hash}")
                else:
                    if status == "already_paused":
                        logger.warning(f"‚ö†Ô∏è CONTRACT ALREADY PAUSED - Address: {contract_address}")
                    elif status == "not_guardian":
                        logger.error(f"‚ùå NOT AUTHORIZED - Account is not a guardian for contract {contract_address}")
                    elif status == "no_contract":
                        logger.error(f"‚ùå NO CONTRACT FOUND - Address: {contract_address}")
                    elif status == "invalid_address":
                        logger.error(f"‚ùå INVALID ADDRESS - Address: {contract_address}")
                    else:
                        logger.error(f"‚ùå FAILED TO PAUSE CONTRACT - Address: {contract_address}, Status: {status}")
            else:
                logger.warning(
                    f"‚ö†Ô∏è PAUSE SKIPPED - No valid contract address configured for query {agg_report.query_id[:16]}..."
                )
        else:
            logger.warning(f"‚ö†Ô∏è PAUSE SKIPPED - No contract address found in config for query {agg_report.query_id[:16]}...")
    else:
        logger.warning(
            "‚ö†Ô∏è PAUSE SKIPPED - Saga contract manager not initialized (check SAGA_EVM_RPC_URL and SAGA_PRIVATE_KEY)"
        )


async def agg_reports_queue_handler(
    agg_reports_q: asyncio.Queue,
    config_watcher: ConfigWatcher,
    logger: logging,
    saga_contract_manager: SagaContractManager | None = None,
    uri: str | None = None,
    power_thresholds: PowerThresholds | None = None,
) -> None:
    """Handle aggregate reports from the queue and check for pause conditions."""
    processed_reports = {}  # Track processed reports with timestamps to detect duplicates
    pending_pauses = {}  # Track pending pauses for delayed power thresholds {query_id: PendingPause}
    last_cleanup_time = time.time()
    last_pending_check = time.time()
    cleanup_interval = 24 * 60 * 60  # 24 hours in seconds
    pending_check_interval = 60  # Check pending pauses every minute

    while True:
        # Check for pending pauses that have reached their delay timeout
        current_time = time.time()
        if current_time - last_pending_check >= pending_check_interval:
            await process_pending_pauses(pending_pauses, saga_contract_manager, logger, current_time)
            last_pending_check = current_time

        agg_report: AggregateReport = await agg_reports_q.get()

        # Create a unique key for this report to detect duplicates (keep for safety)
        report_key = f"{agg_report.query_id}:{agg_report.height}:{agg_report.value}:{agg_report.micro_report_height}"
        current_time = time.time()

        # Periodic cleanup of old processed reports to prevent memory growth
        if current_time - last_cleanup_time >= cleanup_interval:
            initial_count = len(processed_reports)
            cutoff_time = current_time - cleanup_interval  # Remove entries older than 24 hours
            processed_reports = {k: v for k, v in processed_reports.items() if v >= cutoff_time}
            cleaned_count = initial_count - len(processed_reports)
            logger.info(f"üßπ Cleaned up {cleaned_count} old processed report entries (kept {len(processed_reports)})")
            last_cleanup_time = current_time

        if report_key in processed_reports:
            time_diff = current_time - processed_reports[report_key]
            logger.warning(f"üö® DUPLICATE DETECTED! Report already processed {time_diff:.3f}s ago: {report_key}")
            # Skip processing duplicate
            agg_reports_q.task_done()
            continue
        else:
            processed_reports[report_key] = current_time

        # Log the aggregate report with decoded value for readability
        try:
            decoded_value = decode_hex_value(agg_report.value)
            logger.info(
                f"Aggregate Report found - qId: {agg_report.query_id[:16]}... value: {decoded_value:.6f} "
                f"power: {agg_report.aggregate_power} height: {agg_report.height}"
            )
        except (OverflowError, ValueError) as e:
            logger.error(f"üö® Skipping aggregate report - invalid hex value: {e}")
            logger.info(
                f"Aggregate Report found - qId: {agg_report.query_id[:16]}... value: DECODE_ERROR "
                f"power: {agg_report.aggregate_power} height: {agg_report.height}"
            )
            agg_reports_q.task_done()
            continue

        # Inspect aggregate report using same logic as individual reports
        inspection_result = await inspect_aggregate_report(agg_report, config_watcher, logger, uri, power_thresholds)

        if inspection_result:
            # Handle both 2-tuple and 3-tuple return formats for backward compatibility
            if len(inspection_result) == 3:
                should_pause, reason, power_info = inspection_result
            else:
                should_pause, reason = inspection_result
                power_info = None

            # Check if this report should trigger immediate pause or delayed pause
            immediate_pause = should_pause
            delayed_pause = False

            if power_info and not should_pause:
                # Check if this meets delayed pause criteria
                delayed_pause = power_info.get("should_pause_delayed", False)

            if immediate_pause:
                logger.critical(f"üö® CIRCUIT BREAKER ACTIVATED: {reason}")

                # Remove any existing pending pause for this query (superseded by immediate pause)
                if agg_report.query_id in pending_pauses:
                    del pending_pauses[agg_report.query_id]
                    logger.info(f"Removed pending pause for {agg_report.query_id[:16]}... (superseded by immediate pause)")

                # Execute immediate pause
                await execute_contract_pause(saga_contract_manager, agg_report, config_watcher, logger)

            elif delayed_pause:
                logger.critical(f"üïê DELAYED PAUSE SCHEDULED: {reason}")

                # Get contract address for delayed pause
                config = config_watcher.get_config()
                query_config = config.get(agg_report.query_id.lower())

                if query_config and query_config.get("datafeed_ca"):
                    contract_address = query_config.get("datafeed_ca")

                    if contract_address != "0x0000000000000000000000000000000000000000":
                        # Check if there's already a pending pause for this query
                        if agg_report.query_id in pending_pauses:
                            logger.info(f"Pending pause already exists for {agg_report.query_id[:16]}...")
                        else:
                            # Create pending pause
                            pending_pause = PendingPause(
                                query_id=agg_report.query_id,
                                contract_address=contract_address,
                                trigger_time=current_time,
                                power_info=power_info,
                                agg_report=agg_report,
                                reason=reason,
                            )
                            pending_pauses[agg_report.query_id] = pending_pause

                            logger.critical(
                                f"‚è∞ PAUSE SCHEDULED - Will pause {contract_address} "
                                f"for query {agg_report.query_id[:16]}... "
                                f"in {power_info['delay_hours']} hours if not disputed"
                            )
                    else:
                        logger.warning(
                            f"‚ö†Ô∏è DELAYED PAUSE SKIPPED - No valid contract address for query {agg_report.query_id[:16]}..."
                        )
                else:
                    logger.warning(f"‚ö†Ô∏è DELAYED PAUSE SKIPPED - No contract config for query {agg_report.query_id[:16]}...")
            else:
                # Report validated - no action needed
                pass
        else:
            logger.warning("Could not validate aggregate report - configuration or trusted source unavailable")

        agg_reports_q.task_done()


async def inspect_evm_call_reports(
    reports: list[NewReport],
    feed: DataFeed,
    disputes_q: asyncio.Queue,
    query_id: str,
    metrics: Metrics,
    logger: logging,
) -> None:
    """Inspect reports for evm call query type and check if they are disputable.

    Reason they are different is because we need to specifically fetch values for each report
    because each value needs to be decoded and is dependant on the block timestamp in the value
    to fetch the trusted value from the relevant chain.

    reports: list of new_reports for a query id
    disputes_q: the queue to add disputes to for dispute processing
    query_id: the hash of the query data used to fetch the value.
    metrics: the metrics object for the query id
    logger: the logger object
    """
    # Get Web3 connection for the target chain
    chain_id = feed.query.chainId
    logger.info(f"EVMCall handler - inspecting {len(reports)} report(s) for chain_id: {chain_id}, query_id: {query_id}")

    w3 = get_web3_connection(chain_id)
    if w3 is None:
        logger.error(f"Failed to get Web3 connection for chain_id {chain_id}")
        return None

    logger.info(f"EVMCall handler - successfully connected to chain_id: {chain_id}")

    for r in reports:
        # Decode the reported value from hex to get (result_bytes, timestamp) tuple
        try:
            logger.info(
                f"EVMCall report decoded - chain_id: {chain_id}, contract: {feed.query.contractAddress}, value: {r.value}"
            )
            reported_value = feed.query.value_type.decode(bytes.fromhex(r.value))
        except Exception as e:
            logger.error(f"Failed to decode EVMCall reported value for chain_id {chain_id}: {e}")
            continue

        trusted_value = await get_evm_call_trusted_value(reported_value, feed, w3, chain_id)

        if trusted_value is None:
            logger.error(f"unable to fetch trusted value for query id: {query_id}")
            continue

        # EVMCall format: reporters submit abi.encode(abi.encode(value), timestamp)
        # reported_value[0] is abi.encode(value) which is already the raw bytes representation
        # trusted_value from eth_call is also raw bytes
        # Both are 32-byte representations of the contract return value, compare directly
        decoded_reported = reported_value[0]

        logger.info(f"Comparing reported: {decoded_reported.hex()} vs trusted: {trusted_value.hex()}")

        await inspect(r, decoded_reported, trusted_value, disputes_q, metrics, logger)
    return None


async def send_foreign_query_alert(
    query_id: str, query_type: str, asset_pair: str, report: NewReport, logger: logging.Logger
) -> None:
    """Send Discord alert for foreign queries not in our configs."""
    try:
        # Get monitor name from environment
        monitor_name = os.getenv("MONITOR_NAME", "LVM")

        # Extract reported value for display
        reported_value = "Unknown"
        try:
            # Try to decode the reported value
            query_obj = get_query(report.query_data)
            if query_obj:
                reported_value = query_obj.value_type.decode(bytes.fromhex(report.value))
        except Exception:
            # If we can't decode, just show the raw hex value
            reported_value = report.value

        # Build the alert message
        alert_msg = "‚ö†Ô∏è **Query not found in telliot or configs**\n"
        alert_msg += f"**QueryId:** {query_id}\n"
        alert_msg += f"**QueryType:** {query_type}\n"
        if asset_pair != "Unknown":
            alert_msg += f"**Asset pair:** {asset_pair}\n"
        alert_msg += f"**Value:** {reported_value}\n"
        alert_msg += f"**Reporter:** {report.reporter}\n"
        alert_msg += f"**Tx Hash:** {report.tx_hash}"

        logger.info(f"Foreign query alert:\n{alert_msg}")
        generic_alert(alert_msg)

    except Exception as e:
        logger.error(f"Failed to send foreign query alert: {e}")


async def send_unknown_query_type_alert(
    query_type: str, query_id: str, asset_pair: str, report: NewReport, logger: logging.Logger
) -> None:
    """Send Discord alert for unknown query types."""
    try:
        monitor_name = os.getenv("MONITOR_NAME", "LVM")
        reported_value = "Unknown"
        try:
            query_obj = get_query(report.query_data)
            if query_obj:
                reported_value = query_obj.value_type.decode(bytes.fromhex(report.value))
        except Exception:
            reported_value = report.value

        alert_msg = f"**{monitor_name}** does not support query type: {query_type}\n"
        alert_msg += f"**QueryId:** {query_id}\n"
        if asset_pair != "Unknown":
            alert_msg += f"**Asset pair:** {asset_pair}\n"
        alert_msg += f"**Value:** {reported_value}\n"
        alert_msg += f"**Reporter:** {report.reporter}\n"
        alert_msg += f"**Tx Hash:** {report.tx_hash}"

        logger.info(f"Unknown query type alert:\n{alert_msg}")
        generic_alert(alert_msg)
    except Exception as e:
        logger.error(f"Failed to send unknown query type alert: {e}")


async def inspect_trbbridge_reports(
    reports: list[NewReport],
    disputes_q: asyncio.Queue,
    query_id: str,
    metrics: Metrics,
    logger: logging,
) -> None:
    """Inspect reports for TRBBridge query type and check if they are disputable.

    For TRBBridge reports, we decode the deposit ID from queryData, fetch the deposit details
    from the contract's deposits mapping, and compare against the reported values.

    reports: list of new_reports for a query id
    disputes_q: the queue to add disputes to for dispute processing
    query_id: the hash of the query data used to fetch the value.
    metrics: the metrics object for the query id
    logger: the logger object
    """
    # Get TRBBridge configuration from environment variables, default to mainnet if not set
    contract_address = os.getenv("TRBBRIDGE_CONTRACT_ADDRESS")
    chain_id = int(os.getenv("TRBBRIDGE_CHAIN_ID", "1"))
    rpc_url = os.getenv("BRIDGE_CHAIN_RPC_URL")

    if not contract_address:
        logger.error("TRBBRIDGE_CONTRACT_ADDRESS environment variable is not set")
        return None

    logger.info(f"TRBBridge contract address: {contract_address}")
    logger.info(f"chain ID: {chain_id}")

    for r in reports:
        # Decode the reported value
        reported_details = decode_report_value(r.value)
        if reported_details is None:
            logger.error(f"Failed to decode reported value for TRBBridge query {query_id}")
            continue

        reported_eth_addr, reported_layer_addr, reported_amount, reported_tip = reported_details

        # Get trusted value from contract
        trusted_details = await get_trb_bridge_trusted_value(r.query_data, contract_address, chain_id, rpc_url, logger)

        if trusted_details is None:
            logger.error(f"Unable to fetch trusted value for TRBBridge query {query_id}")
            continue

        trusted_eth_addr, trusted_layer_addr, trusted_amount, trusted_tip = trusted_details

        # Create comparison structures for inspection
        reported_value = {
            "eth_address": reported_eth_addr,
            "layer_address": reported_layer_addr,
            "amount": reported_amount,
            "tip": reported_tip,
        }

        trusted_value = {
            "eth_address": trusted_eth_addr,
            "layer_address": trusted_layer_addr,
            "amount": trusted_amount,
            "tip": trusted_tip,
        }

        logger.info(f"TRBBridge Reported: {reported_value}")
        logger.info(f"TRBBridge Trusted: {trusted_value}")

        # Use string comparison for equality check (addresses and layer address)
        # and exact value comparison for amounts
        # Note: We don't need to store the matches result since we use exact equality for TRBBridge

        # For TRBBridge, we expect exact equality
        # Note: TRBBridge doesn't have a standard query object, pass None
        await inspect(r, reported_value, trusted_value, disputes_q, metrics, logger, query=None)

    return None


async def inspect(
    report: NewReport,
    reported_value: Any,
    trusted_value: Any,
    disputes_q: asyncio.Queue,
    metrics: Metrics,
    logger: logging,
    query: Any = None,
    trusted_value_fetcher: Any = None,
) -> None:
    """Inspect a new report and check if it is disputable.
    
    Args:
        trusted_value_fetcher: Optional async callable that returns (value, timestamp) tuple for re-fetching trusted value
    """
    display = {
        "REPORTER": report.reporter,
        "QUERY_TYPE": report.query_type,
        "QUERY_ID": report.query_id,
        "AGGREGATE_METHOD": report.aggregate_method,
        "CYCLELIST": report.cyclelist,
        "POWER": report.power,
        "TIMESTAMP": int(report.timestamp),
        "TRUSTED_VALUE": trusted_value,
        "TX_HASH": report.tx_hash,
    }
    display["CURRENT_TIME"] = int(time.time() * 1000)
    display["TIME_DIFF"] = display["CURRENT_TIME"] - (int(report.timestamp))
    display["VALUE"] = reported_value
    
    # Store first trusted value timestamp
    first_trusted_value = trusted_value
    first_trusted_time = time.time()
    
    # compare values and check against threshold- three metrics(percentage, equality, range)
    alertable, disputable, diff = is_disputable(
        metrics.metric,
        metrics.alert_threshold,
        metrics.warning_threshold,
        reported_value,
        trusted_value,
        logger=logger,
    )
    if alertable is None:
        return None

    # Determine dispute level for Discord alert
    dispute_level = None
    is_disputable_flag = disputable  # Use the disputable value directly

    if disputable:
        if metrics.metric.lower() == "equality":
            if metrics.warning_threshold == 1.0:
                dispute_level = "warning"
            elif metrics.minor_threshold == 1.0:
                dispute_level = "minor"
            elif metrics.major_threshold == 1.0:
                dispute_level = "major"
        else:
            category = determine_dispute_category(
                diff=diff,
                category_thresholds={
                    "major": metrics.major_threshold,
                    "minor": metrics.minor_threshold,
                    "warning": metrics.warning_threshold,
                },
            )
            if category:
                dispute_level = category

    # Double-check logic for SpotPrice queries with trusted_value_fetcher
    second_trusted_value = None
    second_trusted_time = None
    second_check_disputable = None
    should_dispute = False
    dispute_category = None
    
    if disputable and trusted_value_fetcher is not None:
        # First check crossed threshold - initiate double-check
        logger.info(f"‚è≥ First trusted value check crossed threshold (diff: {diff}). Waiting 10 seconds for second check...")
        await asyncio.sleep(10)
        
        logger.info("üì° Fetching second trusted value for verification...")
        try:
            second_trusted_result = await trusted_value_fetcher()
            if second_trusted_result is not None:
                # fetch_value returns (value, timestamp) tuple
                second_trusted_value = second_trusted_result[0] if isinstance(second_trusted_result, tuple) else second_trusted_result
                second_trusted_time = time.time()
                logger.info(f"‚úÖ Second trusted value fetched: {second_trusted_value}")
                
                # Compare reported value against second trusted value
                _, second_check_disputable, second_diff = is_disputable(
                    metrics.metric,
                    metrics.alert_threshold,
                    metrics.warning_threshold,
                    reported_value,
                    second_trusted_value,
                    logger=logger,
                )
                
                # Only dispute if BOTH checks cross threshold
                if second_check_disputable:
                    logger.info(f"‚úÖ Second check also crossed threshold (diff: {second_diff}). Proceeding with dispute.")
                    should_dispute = True
                    
                    # Determine dispute category
                    if metrics.metric.lower() == "equality":
                        if metrics.warning_threshold == 1.0:
                            dispute_category = "warning"
                        elif metrics.minor_threshold == 1.0:
                            dispute_category = "minor"
                        elif metrics.major_threshold == 1.0:
                            dispute_category = "major"
                    else:
                        category = determine_dispute_category(
                            diff=second_diff,
                            category_thresholds={
                                "major": metrics.major_threshold,
                                "minor": metrics.minor_threshold,
                                "warning": metrics.warning_threshold,
                            },
                        )
                        if category is not None:
                            dispute_category = category
                else:
                    logger.info(f"‚ö†Ô∏è Second check did NOT cross threshold (diff: {second_diff}). Dispute cancelled.")
            else:
                logger.error("‚ùå Failed to fetch second trusted value. Cancelling dispute.")
        except Exception as e:
            logger.error(f"‚ùå Error fetching second trusted value: {e}. Cancelling dispute.")
    
    elif disputable and trusted_value_fetcher is None:
        # No fetcher provided - use original single-check logic (EVMCall, TRBBridge)
        logger.info(f"disputable: true, diff: {diff} (single-check mode)")
        
        # For equality metrics, determine dispute level based on which threshold is set to 1.0
        if metrics.metric.lower() == "equality":
            if metrics.warning_threshold == 1.0:
                dispute_category = "warning"
                should_dispute = True
                logger.info("Auto-disputing equality mismatch at warning level")
            elif metrics.minor_threshold == 1.0:
                dispute_category = "minor"
                should_dispute = True
                logger.info("Auto-disputing equality mismatch at minor level")
            elif metrics.major_threshold == 1.0:
                dispute_category = "major"
                should_dispute = True
                logger.info("Auto-disputing equality mismatch at major level")
            else:
                logger.debug("No dispute level configured for equality metric (no threshold set to 1.0)")
        else:
            # For other metrics, use traditional threshold-based logic
            category = determine_dispute_category(
                diff=diff,
                category_thresholds={
                    "major": metrics.major_threshold,
                    "minor": metrics.minor_threshold,
                    "warning": metrics.warning_threshold,
                },
            )
            if category is not None:
                dispute_category = category
                should_dispute = True

    # Send Discord alert AFTER double-check completes (or immediately if no double-check)
    if alertable or (disputable and trusted_value_fetcher is not None):
        import os
        from datetime import datetime

        from layer_values_monitor.discord import build_alert_message, format_difference, format_values
        from layer_values_monitor.dispute import get_disputer_address, validate_keyring_config
        from layer_values_monitor.telliot_feeds import extract_query_info

        query_info = extract_query_info(query, query_type=report.query_type)
        diff_str = format_difference(diff, metrics.metric)
        
        # Build message based on whether we did double-check
        if second_trusted_value is not None:
            # Double-check was performed - custom message format
            first_time_str = datetime.fromtimestamp(first_trusted_time).strftime('%Y-%m-%d %H:%M:%S')
            second_time_str = datetime.fromtimestamp(second_trusted_time).strftime('%Y-%m-%d %H:%M:%S')
            
            if should_dispute:
                alert_header = "üö® **Second inspection triggered: sending dispute, reported value outside both trusted values**\n\n"
            else:
                alert_header = "‚ö†Ô∏è **Second inspection triggered: reported value was past the first trusted value, but not the second**\n\n"
            
            # Build custom message
            msg = alert_header
            msg += f"**Asset:** {query_info}\n"
            if report.query_type:
                msg += f"**QueryType:** {report.query_type}\n"
            if dispute_level:
                msg += f"**Level:** {dispute_level}\n"
            msg += f"**Reported Value:** {reported_value}\n"
            msg += f"**First Trusted Value:** {first_trusted_value} (retrieved at {first_time_str})\n"
            msg += f"**Second Trusted Value:** {second_trusted_value} (retrieved at {second_time_str})\n"
            msg += f"**First Check Difference:** {diff_str}\n"
            if second_check_disputable is not None:
                second_diff_str = format_difference(second_diff, metrics.metric)
                msg += f"**Second Check Difference:** {second_diff_str}\n"
            msg += f"**Reporter:** {report.reporter}\n"
            msg += f"**Power:** {report.power}\n"
            msg += f"**Tx Hash:** {report.tx_hash}\n"
            
            # Add disputer info if disputing
            if should_dispute and dispute_level != "warning":
                binary_path = os.getenv("LAYER_BINARY_PATH")
                key_name = os.getenv("LAYER_KEY_NAME")
                kb = os.getenv("LAYER_KEYRING_BACKEND")
                kdir = os.getenv("LAYER_KEYRING_DIR")

                if binary_path and key_name and kb and kdir:
                    if validate_keyring_config(binary_path, key_name, kb, kdir):
                        disputer_address = get_disputer_address(binary_path, key_name, kb, kdir)
                        if disputer_address:
                            msg += f"**Disputer:** {disputer_address}, {key_name}\n"
                    else:
                        msg += f"**Disputer:** {key_name} improperly configured, no dispute sent\n"
            
            logger.info(f"Double-check alert:\n{msg}")
            generic_alert(msg)
            
        else:
            # Standard single-check alert (no fetcher or not disputable)
            value_display = format_values(reported_value, trusted_value)

            # Get disputer information based on dispute level and keyring config
            disputer_info = None
            if is_disputable_flag and dispute_level != "warning":
                # Only show disputer info for disputable alerts (not warnings)
                binary_path = os.getenv("LAYER_BINARY_PATH")
                key_name = os.getenv("LAYER_KEY_NAME")
                kb = os.getenv("LAYER_KEYRING_BACKEND")
                kdir = os.getenv("LAYER_KEYRING_DIR")

                if binary_path and key_name and kb and kdir:
                    # Check if keyring is properly configured
                    if validate_keyring_config(binary_path, key_name, kb, kdir):
                        disputer_address = get_disputer_address(binary_path, key_name, kb, kdir)
                        if disputer_address:
                            disputer_info = f"{disputer_address}, {key_name}"
                    else:
                        disputer_info = f"{key_name} improperly configured, no dispute sent"

            msg = build_alert_message(
                query_info=query_info,
                value_display=value_display,
                diff_str=diff_str,
                reporter=report.reporter,
                power=report.power,
                tx_hash=report.tx_hash,
                query_type=report.query_type,
                disputer_info=disputer_info,
                level=dispute_level,
            )

            logger.info(f"Alertable value detected:\n{msg}")
            generic_alert(msg)

    display["DISPUTABLE"] = disputable

    # Submit dispute if needed
    if should_dispute and dispute_category is not None:
        logger.info(f"Getting ready to send dispute tx... reported value: {reported_value}")
        fee = determine_dispute_fee(dispute_category, int(report.power))
        logger.info(f"Fee: {fee.__str__() + DENOM}")
        await disputes_q.put(Msg(report.reporter, report.query_id, report.meta_id, dispute_category, fee.__str__() + DENOM))
    add_to_table(display)
